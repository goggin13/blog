# Backup files to S3 from Ruby

I'm paranoid.  As such, I like to back things up.  In more than one place, if possible.  Most often, my backup schemes involve:  
* pushing code to Github  
* nightly backups to an external hard drive via OSX's Time Machine  

Since returning to grad school this has proved problematic, as I'm doing lots of work that I'm *not* storing in Github.  I have a paid account on github, which provides 5 private repos, but I'm already using most of them, and I don't want to just dump everything I want to back up into a single repo.  I have a directory structure on my laptop that I like and want to maintain.  

So these files that are not on github still get backed up nightly; but since I'm paranoid this still bothers me for 2 reasons:  
* My external hard drive lives in the same room as my laptop when I'm home.  Should bad things happen to my apartment (theft, fire, zombies), it will most likely not survive if the laptop does not.  
* I very often do work away from home; in these situations, I pack up my laptop and walk home (or go away for the weekend, whatever), and my most recent work remains not backed up until I return home to plug it into my external HD.  What if I slip and fall!? Or am kidnapped (by zombies)!? Unacceptable.

I did some quick googling for other solutions online.  I saw some other existing solutions that use S3, but the two that sounded the most promising gave me 404's back when I tried to download their scripts :(.  There is a good looking paid solution in [Dolly Drive](http://www.dollydrive.com/), for as low as $3 a month.  If I didn't just feel like building this for fun, I would go that route.  In addition, it will give you the full time machine backup functionality, which my code will not.  

## My goals  
* Write a ruby script that reads a config file and backs up selected directories to S3.  
* It should be smart enough to not re-upload unmodified files.  
* It should be configurable to only read certain directories, and ignore some files.  
* Write it in the next hour and half before class because I should really be doing other stuff  

## Results  
While it took longer than the hour and a half (more like 3 hrs) before class, I think I have a decent solution.  The code is here [on Github](https://github.com/goggin13/s3backup).  I'm currently backing up files in certain directories to S3 (woo hoo!!) and my fire and zombie related paranoia is subsiding.  

I'm intending this backup to be just for things I really don't want to lose.  It's **not** meant to replace my Time Machine backups; just to keep the files I really want in case something happens to my external hard disk.  Additionally, once I've uploaded the initial backup, it should be fast to incrementally push files so I can solve the problem of conveniently backing up my work when I'm away from home.  

## Nice to haves  (todo when bored)  
* Also delete files in S3 that are no longer present in the directories to be backed up  
* Since the bottle neck in pushing all these files is the single thread waiting on the network, I believe I could get a significant speed up by assigning a few more threads to run at the same time, sharing a queue of files to upload.  
* The initial upload could be more efficient if we used the --recursive option of the s3 wrapper I'm using.  This will require different methods when doing initial backup and incremental backups, which is why it didn't happen in round I.  

## Did I save money?  
I uploaded (am uploading) ~35 gig of data.  This includes my music, pictures and a lot of documents (not movies in my itunes though).  The data is spread over ~55000 files.  

Amazon charges $0.01 per thousand requests, (but no additional charge per byte for pushing data up), and since my script goes file by file (so that later I can only upload changed files), that is   
`55,000 / 1000 * 0.01 = $0.55`  first time cost.  

Storage is $0.125 per GB for the first TB of data. I am still uploading all my data (I'm compressing them as I go), but once that is done I'll update here and compare my costs against DollyDrive plans.  

## You should use this script...  
If you are a ruby programmer who wants to be in complete control of your backup scheme, and also perhaps to save a few dollars vs. other hosted methods.  

 
# Backup files to S3 from Ruby

I'm paranoid.  As such, I like to back things up.  In more than one place, if possible.  Most often, my backup schemes involve:  
* pushing code to Github  
* nightly backups to an external hard drive via OSX's Time Machine  

Since returning to grad school this has proved problematic, as I'm doing lots of work that I'm *not* storing in Github.  I have a paid account on github, which provides 5 private repos, but I'm already using most of them, and I don't want to just dump everything I want to back up into a single repo.  I have a directory structure on my laptop that I like and want to maintain.  

So these files that are not on github still get backed up nightly; but since I'm paranoid this still bothers me for 2 reasons:  
* My external hard drive lives in the same room as my laptop when I'm home.  Should bad things happen to my apartment (theft, fire, zombies), it will most likely not survive if the laptop does not.  
* I very often do work away from home; in these situations, I pack up my laptop and walk home (or go away for the weekend, whatever), and my most recent work remains not backed up until I return home to plug it into my external HD.  What if I slip and fall!? Or am kidnapped (by zombies)!? Unacceptable.

I did some quick googling for other solutions online.  I saw some other existing solutions that use S3, but the two that sounded the most promising gave me 404's back when I tried to download their scripts :(.  There is a good looking paid solution in [Dolly Drive](http://www.dollydrive.com/), for as low as $3 a month.  If I didn't just feel like building this for fun, I would go that route.  In addition, it will give you the full time machine backup functionality, which my code will not.  

## My goals  
* Write a ruby script that reads a config file and backs up selected directories to S3.  
* It should be smart enough to not re-upload unmodified files.  
* It should be configurable to only read certain directories, and ignore some files.  
* Write it in the next hour and half before class because I should really be doing other stuff  

## Results  
While it took longer than the hour and a half (more like 3 hrs) before class, I think I have a decent solution.  The code is here [on Github](https://github.com/goggin13/s3backup).  I'm currently backing up files in certain directories to S3 (woo hoo!!) and my fire and zombie related paranoia is subsiding.  

I'm intending this backup to be just for things I really don't want to lose.  It's **not** meant to replace my Time Machine backups; just to keep the files I really want in case something happens to my external hard disk.  Additionally, once I've uploaded the initial backup, it should be fast to incrementally push files so I can solve the problem of conveniently backing up my work when I'm away from home.  

## Nice to haves  (todo when bored)  
* Also delete files in S3 that are no longer present in the directories to be backed up  
* Since the bottle neck in pushing all these files is the single thread waiting on the network, I believe I could get a significant speed up by assigning a few more threads to run at the same time, sharing a queue of files to upload.  
* The initial upload could be more efficient if we used the --recursive option of the s3 wrapper I'm using.  This will require different methods when doing initial backup and incremental backups, which is why it didn't happen in round I.  

## Did I save money?  
I uploaded (am uploading) ~35 gig of data.  This includes my music, pictures and a lot of documents (not movies in my itunes though).  The data is spread over ~55000 files.  

Amazon charges $0.01 per thousand requests, (but no additional charge per byte for pushing data up), and since my script goes file by file (so that later I can only upload changed files), that is   
`55,000 / 1000 * 0.01 = $0.55`  first time cost.  

Storage is $0.125 per GB for the first TB of data. I am still uploading all my data (I'm compressing them as I go), but once that is done I'll update here and compare my costs against DollyDrive plans.  

## You should use this script...  
If you are a ruby programmer who wants to be in complete control of your backup scheme, and also perhaps to save a few dollars vs. other hosted methods.  

 
# Backup files to S3 from Ruby

I'm paranoid.  As such, I like to back things up.  In more than one place, if possible.  Most often, my backup schemes involve:  
* pushing code to Github  
* nightly backups to an external hard drive via OSX's Time Machine  

Since returning to grad school this has proved problematic, as I'm doing lots of work that I'm *not* storing in Github.  I have a paid account on github, which provides 5 private repos, but I'm already using most of them, and I don't want to just dump everything I want to back up into a single repo.  I have a directory structure on my laptop that I like and want to maintain.  

So these files that are not on github still get backed up nightly; but since I'm paranoid this still bothers me for 2 reasons:  
* My external hard drive lives in the same room as my laptop when I'm home.  Should bad things happen to my apartment (theft, fire, zombies), it will most likely not survive if the laptop does not.  
* I very often do work away from home; in these situations, I pack up my laptop and walk home (or go away for the weekend, whatever), and my most recent work remains not backed up until I return home to plug it into my external HD.  What if I slip and fall!? Or am kidnapped (by zombies)!? Unacceptable.

I did some quick googling for other solutions online.  I saw some other existing solutions that use S3, but the two that sounded the most promising gave me 404's back when I tried to download their scripts :(.  There is a good looking paid solution in [Dolly Drive](http://www.dollydrive.com/), for as low as $3 a month.  If I didn't just feel like building this for fun, I would go that route.  In addition, it will give you the full time machine backup functionality, which my code will not.  

## My goals  
* Write a ruby script that reads a config file and backs up selected directories to S3.  
* It should be smart enough to not re-upload unmodified files.  
* It should be configurable to only read certain directories, and ignore some files.  
* Write it in the next hour and half before class because I should really be doing other stuff  

## Results  
While it took longer than the hour and a half (more like 3 hrs) before class, I think I have a decent solution.  The code is here [on Github](https://github.com/goggin13/s3backup).  I'm currently backing up files in certain directories to S3 (woo hoo!!) and my fire and zombie related paranoia is subsiding.  

I'm intending this backup to be just for things I really don't want to lose.  It's **not** meant to replace my Time Machine backups; just to keep the files I really want in case something happens to my external hard disk.  Additionally, once I've uploaded the initial backup, it should be fast to incrementally push files so I can solve the problem of conveniently backing up my work when I'm away from home.  

## Nice to haves  (todo when bored)  
* Also delete files in S3 that are no longer present in the directories to be backed up  
* Since the bottle neck in pushing all these files is the single thread waiting on the network, I believe I could get a significant speed up by assigning a few more threads to run at the same time, sharing a queue of files to upload.  
* The initial upload could be more efficient if we used the --recursive option of the s3 wrapper I'm using.  This will require different methods when doing initial backup and incremental backups, which is why it didn't happen in round I.  

## Did I save money?  
I uploaded (am uploading) ~35 gig of data.  This includes my music, pictures and a lot of documents (not movies in my itunes though).  The data is spread over ~55000 files.  

Amazon charges $0.01 per thousand requests, (but no additional charge per byte for pushing data up), and since my script goes file by file (so that later I can only upload changed files), that is   
`55,000 / 1000 * 0.01 = $0.55`  first time cost.  

Storage is $0.125 per GB for the first TB of data. I am still uploading all my data (I'm compressing them as I go), but once that is done I'll update here and compare my costs against DollyDrive plans.  

## You should use this script...  
If you are a ruby programmer who wants to be in complete control of your backup scheme, and also perhaps to save a few dollars vs. other hosted methods.  

 
# Backup files to S3 from Ruby

I'm paranoid.  As such, I like to back things up.  In more than one place, if possible.  Most often, my backup schemes involve:  
* pushing code to Github  
* nightly backups to an external hard drive via OSX's Time Machine  

Since returning to grad school this has proved problematic, as I'm doing lots of work that I'm *not* storing in Github.  I have a paid account on github, which provides 5 private repos, but I'm already using most of them, and I don't want to just dump everything I want to back up into a single repo.  I have a directory structure on my laptop that I like and want to maintain.  

So these files that are not on github still get backed up nightly; but since I'm paranoid this still bothers me for 2 reasons:  
* My external hard drive lives in the same room as my laptop when I'm home.  Should bad things happen to my apartment (theft, fire, zombies), it will most likely not survive if the laptop does not.  
* I very often do work away from home; in these situations, I pack up my laptop and walk home (or go away for the weekend, whatever), and my most recent work remains not backed up until I return home to plug it into my external HD.  What if I slip and fall!? Or am kidnapped (by zombies)!? Unacceptable.

I did some quick googling for other solutions online.  I saw some other existing solutions that use S3, but the two that sounded the most promising gave me 404's back when I tried to download their scripts :(.  There is a good looking paid solution in [Dolly Drive](http://www.dollydrive.com/), for as low as $3 a month.  If I didn't just feel like building this for fun, I would go that route.  In addition, it will give you the full time machine backup functionality, which my code will not.  

## My goals  
* Write a ruby script that reads a config file and backs up selected directories to S3.  
* It should be smart enough to not re-upload unmodified files.  
* It should be configurable to only read certain directories, and ignore some files.  
* Write it in the next hour and half before class because I should really be doing other stuff  

## Results  
While it took longer than the hour and a half (more like 3 hrs) before class, I think I have a decent solution.  The code is here [on Github](https://github.com/goggin13/s3backup).  I'm currently backing up files in certain directories to S3 (woo hoo!!) and my fire and zombie related paranoia is subsiding.  

I'm intending this backup to be just for things I really don't want to lose.  It's **not** meant to replace my Time Machine backups; just to keep the files I really want in case something happens to my external hard disk.  Additionally, once I've uploaded the initial backup, it should be fast to incrementally push files so I can solve the problem of conveniently backing up my work when I'm away from home.  

## Nice to haves  (todo when bored)  
* Also delete files in S3 that are no longer present in the directories to be backed up  
* Since the bottle neck in pushing all these files is the single thread waiting on the network, I believe I could get a significant speed up by assigning a few more threads to run at the same time, sharing a queue of files to upload.  
* The initial upload could be more efficient if we used the --recursive option of the s3 wrapper I'm using.  This will require different methods when doing initial backup and incremental backups, which is why it didn't happen in round I.  

## Did I save money?  
I uploaded (am uploading) ~35 gig of data.  This includes my music, pictures and a lot of documents (not movies in my itunes though).  The data is spread over ~55000 files.  

Amazon charges $0.01 per thousand requests, (but no additional charge per byte for pushing data up), and since my script goes file by file (so that later I can only upload changed files), that is   
`55,000 / 1000 * 0.01 = $0.55`  first time cost.  

Storage is $0.125 per GB for the first TB of data. I am still uploading all my data (I'm compressing them as I go), but once that is done I'll update here and compare my costs against DollyDrive plans.  

## You should use this script...  
If you are a ruby programmer who wants to be in complete control of your backup scheme, and also perhaps to save a few dollars vs. other hosted methods.  

 
